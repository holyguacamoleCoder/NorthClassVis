{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  手动实现 PCA 主成分分析\n",
    "本 Notebook 用于学习PCA，一步步拆解 PCA 算法的实现过程，并解释其背后的数学原理。\n",
    "\n",
    "###  PCA 是什么？\n",
    "- PCA（Principal Component Analysis）是一种常用的无监督降维技术。\n",
    "- 它通过找到数据中方差最大的方向（主成分），将高维数据投影到低维空间。\n",
    "- 目标：保留尽可能多的信息（方差），同时减少特征数量。\n",
    "\n",
    "###  核心步骤回顾\n",
    "1. 数据标准化（中心化）\n",
    "2. 计算协方差矩阵\n",
    "3. 特征值分解 → 得到主成分方向\n",
    "4. 投影到前 n 个主成分空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 1: 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据 X shape: (150, 4)\n",
      "前5个样本:\n",
      " [[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# 加载鸢尾花数据集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "\n",
    "print(\"原始数据 X shape:\", X.shape)\n",
    "print(\"前5个样本:\\n\", X[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 2: 数据标准化（中心化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据中心化后的 shape: (150, 4)\n",
      "前5行样本:\n",
      " [[-0.74333333  0.446      -2.35866667 -0.99866667]\n",
      " [-0.94333333 -0.054      -2.35866667 -0.99866667]\n",
      " [-1.14333333  0.146      -2.45866667 -0.99866667]\n",
      " [-1.24333333  0.046      -2.25866667 -0.99866667]\n",
      " [-0.84333333  0.546      -2.35866667 -0.99866667]]\n"
     ]
    }
   ],
   "source": [
    "# 计算每个特征的均值\n",
    "mean = np.mean(X, axis=0)\n",
    "\n",
    "# 去中心化（减去均值）\n",
    "X_centered = X - mean\n",
    "\n",
    "print(\"数据中心化后的 shape:\", X_centered.shape)\n",
    "print(\"前5行样本:\\n\", X_centered[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 3: 计算协方差矩阵\n",
    "协方差矩阵描述了各特征之间的线性关系，PCA 利用它来找出数据变化最大的方向。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cov shape: (4, 4)\n",
      "协方差矩阵:\n",
      " [[ 0.68569351 -0.03926846  1.27368233  0.5169038 ]\n",
      " [-0.03926846  0.18800403 -0.32171275 -0.11798121]\n",
      " [ 1.27368233 -0.32171275  3.11317942  1.29638747]\n",
      " [ 0.5169038  -0.11798121  1.29638747  0.58241432]]\n"
     ]
    }
   ],
   "source": [
    "# 计算协方差矩阵\n",
    "cov_matrix = np.cov(X_centered, rowvar=False)\n",
    "\n",
    "print(\"cov shape:\", cov_matrix.shape)\n",
    "print(\"协方差矩阵:\\n\", cov_matrix) # 4个特征，就是4*4的对称矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 4: 特征值分解\n",
    "协方差矩阵是一个对称矩阵，我们可以对其做特征值分解，其中：\n",
    "- 特征值表示该方向上的方差大小（重要性）\n",
    "- 特征向量就是我们想要找的主成分方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值（排序前）:\n",
      " [0.02368303 0.07852391 0.24224357 4.22484077]\n",
      "特征向量（排序前）:\n",
      " [[ 0.31725455  0.58099728  0.65653988 -0.36158968]\n",
      " [-0.32409435 -0.59641809  0.72971237  0.08226889]\n",
      " [-0.47971899 -0.07252408 -0.1757674  -0.85657211]\n",
      " [ 0.75112056 -0.54906091 -0.07470647 -0.35884393]]\n"
     ]
    }
   ],
   "source": [
    "# 特征值分解\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "print(\"特征值（排序前）:\\n\", eigenvalues)\n",
    "print(\"特征向量（排序前）:\\n\", eigenvectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 5: 排序并选择主成分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特征值排序索引： [3 2 1 0]\n",
      "主成分（特征向量）shape: (4, 2)\n",
      "主成分方向:\n",
      " [[-0.36158968  0.65653988]\n",
      " [ 0.08226889  0.72971237]\n",
      " [-0.85657211 -0.1757674 ]\n",
      " [-0.35884393 -0.07470647]]\n"
     ]
    }
   ],
   "source": [
    "# 按特征值从大到小排序\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "print(\"特征值排序索引：\", sorted_indices)\n",
    "\n",
    "# 选择前 n_components 个主成分\n",
    "n_components = 2\n",
    "components = eigenvectors[:, sorted_indices[:n_components]]\n",
    "\n",
    "print(\"主成分（特征向量）shape:\", components.shape)\n",
    "print(\"主成分方向:\\n\", components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 6: 数据投影到主成分空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "降维后数据 shape: (150, 2)\n",
      "前5个样本投影结果:\n",
      " [[ 2.68420713  0.32660731]\n",
      " [ 2.71539062 -0.16955685]\n",
      " [ 2.88981954 -0.13734561]\n",
      " [ 2.7464372  -0.31112432]\n",
      " [ 2.72859298  0.33392456]]\n"
     ]
    }
   ],
   "source": [
    "# 将数据投影到主成分方向上\n",
    "# 点积（np.dot）\n",
    "X_pca = np.dot(X_centered, components)\n",
    "\n",
    "print(\"降维后数据 shape:\", X_pca.shape)\n",
    "print(\"前5个样本投影结果:\\n\", X_pca[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: 对比 sklearn 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 sklearn 的 PCA 进行对比\n",
    "sk_pca = PCA(n_components=2)\n",
    "X_sk = sk_pca.fit_transform(X)\n",
    "\n",
    "print(\"自定义 PCA 结果前5行:\\n\", X_pca[:5])\n",
    "print(\"sklearn PCA 结果前5行:\\n\", X_sk[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA的数学解释\n",
    "\n",
    "可以用于推导结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一、 为什么要做协方差矩阵 + 特征值分解？\n",
    "1. 目标函数\n",
    "PCA 的目标是寻找一个单位向量 $ w $，使得数据在该方向上的投影方差最大： $$ \\max_w \\frac{1}{n} \\sum_{i=1}^n (w^T x_i)^2 = w^T S w $$ 其中 $ S $ 是数据的协方差矩阵。\n",
    "\n",
    "2. 拉格朗日乘子法求极值\n",
    "引入约束 $ w^T w = 1 $，构造拉格朗日函数： $$ L(w) = w^T S w - λ(w^T w - 1) $$ 对 $ w $ 求导并令导数为零，得： $$ S w = λ w $$ 这就是标准的特征值问题。\n",
    "\n",
    "3. 结论\n",
    "最大方差对应的方向就是协方差矩阵的最大特征值对应的特征向量。\n",
    "所有主成分就是按特征值大小排列的特征向量。\n",
    "因此，PCA 的本质是：找到数据中方差最大的方向（主成分）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 二、为什么 PCA 的目标函数是最大化投影数据的方差？\n",
    "\n",
    "$$ \\max_w \\frac{1}{n} \\sum_{i=1}^n (w^T x_i)^2 = w^T S w $$\n",
    "\n",
    "其中：\n",
    "\n",
    "$ w $ 是一个单位向量（主成分方向）；\n",
    "$ x_i $ 是原始数据点；\n",
    "$ S $ 是数据的协方差矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight1: PCA 的核心思想：保留最大信息**\n",
    "\n",
    "PCA 的目标是降维，即在尽可能保留原始数据信息的前提下，把高维数据映射到低维空间中。\n",
    "那么，“信息”在这里指的是什么？\n",
    "\n",
    "---答： 方差。\n",
    "\n",
    "- 数据的方差越大，说明该方向上包含的信息越多。\n",
    "- 如果你把数据投影到一个方差很小的方向上，很多样本会挤在一起，无法区分。\n",
    "- 所以 PCA 的目标是：找到一个方向（向量），使得数据在这个方向上的投影具有最大的方差。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight2: 投影后的方差怎么计算？**\n",
    "\n",
    "假设我们有以下数据：\n",
    "\n",
    "数据矩阵 $ X \\in \\mathbb{R}^{n \\times d} $：共 $ n $ 个样本，每个样本 $ d $ 维；\n",
    "\n",
    "每个样本为行向量 $ x_i \\in \\mathbb{R}^d $；\n",
    "\n",
    "我们选择一个单位向量 $ w \\in \\mathbb{R}^d $，表示我们要投影的方向。\n",
    "\n",
    "将每个样本 $ x_i $ 投影到 $ w $ 上得到一个标量值： $$ z_i = w^T x_i $$\n",
    "\n",
    "所有样本投影后的方差为： $$ \\text{Var}(z) = \\frac{1}{n} \\sum_{i=1}^n z_i^2 - \\left( \\frac{1}{n} \\sum_{i=1}^n z_i \\right)^2 $$\n",
    "\n",
    "如果数据已经中心化（即均值为0），那么第二项为0，所以： $$ \\text{Var}(z) = \\frac{1}{n} \\sum_{i=1}^n (w^T x_i)^2 $$\n",
    "\n",
    "这就是我们优化的目标函数： $$ \\max_w \\frac{1}{n} \\sum_{i=1}^n (w^T x_i)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight3: 用矩阵形式简化表达式**\n",
    "\n",
    "我们可以将上面的目标函数改写为矩阵形式：\n",
    "\n",
    "$$ \\frac{1}{n} \\sum_{i=1}^n (w^T x_i)^2 = \\frac{1}{n} \\sum_{i=1}^n w^T x_i x_i^T w =w^T \\left( \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T \\right) w = w^T S w $$\n",
    "\n",
    "  其中： $$ S = \\frac{1}{n} \\sum_{i=1}^n x_i x_i^T = \\frac{1}{n} X^T X $$ 就是数据的协方差矩阵（前提是数据已中心化）。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
